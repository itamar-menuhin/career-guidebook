---
kind: focus_area
id: ai-safety
title: AI Safety
summary: AI safety research aims to ensure that advanced AI systems remain beneficial and aligned with human values.
role_shapes:
  - Technical Alignment Researcher - working on core alignment problems like reward modeling, scalable oversight, interpretability
  - ML Engineer at Safety Lab - implementing and testing alignment techniques in large models
  - Governance & Policy - shaping AI regulation, international coordination, corporate policies
  - Field Builder - running programs, funding, communications to grow the safety ecosystem
  - Evaluations & Red-teaming - assessing model capabilities and finding failure modes
fit_signals:
  - You find yourself naturally drawn to thinking about worst-case scenarios and how to prevent them
  - You have a strong technical background (ML, math, CS) or policy experience
  - You can maintain motivation working on problems with uncertain or long-term payoffs
  - You're comfortable with the philosophical complexity of defining "beneficial AI"
  - You thrive in research environments with high autonomy
people_to_talk_to:
  - Who is doing alignment work most similar to what you want to do?
  - Is there someone who recently transitioned into AI safety from a comparable background?
  - Who can give feedback on your first small steps?
common_confusions:
  - Assuming all AI governance roles are DC-based policy jobs
  - Thinking you need a PhD before contributing to alignment research
---

AI safety research aims to ensure that advanced AI systems remain beneficial and aligned with human values. The field spans technical research (alignment, interpretability, robustness), governance (policy, coordination), and field-building (education, community).
