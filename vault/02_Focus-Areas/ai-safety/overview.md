Cause area: AI Safety quick-start (v1)
What it’s trying to achieve (plain language)
Help ensure advanced AI systems don’t cause catastrophic harm, by improving how we build/evaluate/govern/deploy powerful AI (technical safety + governance/policy).
What kinds of work exist here
Technical research (alignment, interpretability, evals, robustness)
Research engineering (tools, eval pipelines, infra for safety)
Governance / policy / strategy
Security / preparedness / model risk
Ops / programs / field-building
Communications / writing
What a good fit often looks like (signals)
Motivated by high-stakes downside risk + can tolerate uncertainty/debate
Drawn to either: technical challenge, governance complexity, or enabling infrastructure/community
OK with long feedback loops (or choose Experiment C to make it concrete fast)
Experiments (canonical picks + optional alt)
## Experiment A — 1-hour overview (choose 1)
Robert Miles — “Intro to AI Safety, Remastered” (video)
DeepMind — “AGI Safety Course” playlist (video course)
Optional alt if they prefer reading: 80,000 Hours AI safety researcher career review
## Experiment B — longer exposure (choose 1)
AGI Safety Fundamentals (reading group + discussion + project)
BlueDot Impact AI courses (technical safety / governance)
## Experiment C — project pathway
SPAR — a part-time remote program explicitly structured around impactful research projects with mentors in AI safety/policy.
Optional alt (bigger commitment): MATS (intensive mentored research program)
MATS: https://matsprogram.org/
Toe-in-the-water step: pick 2–3 projects you could imagine doing; write 2 lines each (interest + feasibility)
## Experiment D — job-board exploration (choose 1–2)
80,000 Hours job board
AISafety.com jobs
Optional alt: EA Opportunities board
Tiny scanning method (10–25 min): collect 8–12 interesting roles → label role-types → circle 3 roles + 2 repeating skill gaps.
