---
kind: focus_area
id: ai-safety
title: AI Safety quick-start (v1)
summary: Help ensure advanced AI systems don’t cause catastrophic harm, by improving
  how we build/evaluate/govern/deploy powerful AI (technical safety + governance/policy).
role_shapes:
- Technical research (alignment, interpretability, evals, robustness)
- Research engineering (tools, eval pipelines, infra for safety)
- Governance / policy / strategy
- Security / preparedness / model risk
- Ops / programs / field-building
- Communications / writing
fit_signals:
- Motivated by high-stakes downside risk + can tolerate uncertainty/debate
- 'Drawn to either: technical challenge, governance complexity, or enabling infrastructure/community'
- OK with long feedback loops (or choose Experiment C to make it concrete fast)
people_to_talk_to: []
common_confusions: []
---

## What it’s trying to achieve (plain language)
Help ensure advanced AI systems don’t cause catastrophic harm, by improving how we build/evaluate/govern/deploy powerful AI (technical safety + governance/policy).

## What kinds of work exist here (role shapes)
- Technical research (alignment, interpretability, evals, robustness)
- Research engineering (tools, eval pipelines, infra for safety)
- Governance / policy / strategy
- Security / preparedness / model risk
- Ops / programs / field-building
- Communications / writing

## What a good fit often looks like (signals)
- Motivated by high-stakes downside risk + can tolerate uncertainty/debate
- Drawn to either: technical challenge, governance complexity, or enabling infrastructure/community
- OK with long feedback loops (or choose Experiment C to make it concrete fast)

## Experiments (A–D)
Quick taste (≈1 hour) (choose 1)
Robert Miles — “Intro to AI Safety, Remastered” (video)
DeepMind — “AGI Safety Course” playlist (video course)
Optional alt if they prefer reading: 80,000 Hours AI safety researcher career review
Deeper dive (2–6 hours) (choose 1)
AGI Safety Fundamentals (reading group + discussion + project)
BlueDot Impact AI courses (technical safety / governance)
Hands-on trial
SPAR — a part-time remote program explicitly structured around impactful research projects with mentors in AI safety/policy.
Optional alt (bigger commitment): MATS (intensive mentored research program)
MATS: https://matsprogram.org/
First small step (≤60 min) step: pick 2–3 projects you could imagine doing; write 2 lines each (interest + feasibility)
Job board scan (real roles) (choose 1–2)
80,000 Hours job board
AISafety.com jobs
Optional alt: EA Opportunities board
Tiny scanning method (10–25 min): collect 8–12 interesting roles → label role-types → circle 3 roles + 2 repeating skill gaps.

_See the bucket pages for a cleaner A/B/C/D layout, and the Cards section for curated resources._
