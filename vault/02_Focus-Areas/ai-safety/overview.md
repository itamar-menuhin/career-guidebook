---
kind: focus_area
id: ai-safety
title: AI Safety
summary: "Help ensure advanced AI systems don't cause catastrophic harm, by improving how we build/evaluate/govern/deploy powerful AI (technical safety + governance/policy)."
role_shapes:
- "Technical research (alignment, interpretability, evals, robustness)"
- "Research engineering (tools, eval pipelines, infra for safety)"
- "Governance / policy / strategy"
fit_signals:
- "Motivated by catastrophic risk (high-stakes failure modes)"
- "Strong technical / conceptual / strategic skills"
- "Comfortable with significant uncertainty and high abstraction"
common_confusions:
- "Add common confusion here..."
---

Help ensure advanced AI systems don't cause catastrophic harm, by improving how we build/evaluate/govern/deploy powerful AI (technical safety + governance/policy).


