---
kind: card
id: alignment-forum-reading
title: Alignment Forum Deep Dives
focus_area_id: ai-safety
bucket: quick-taste
topic: reading
commitment: low
good_fit_if:
  - Comfortable with technical writing
  - Self-directed learner
  - Curious about research debates
one_liner: Read seminal posts on the Alignment Forum to understand current research debates.
first_small_step: Read "AGI Safety from First Principles" by Richard Ngo (45 min)
next_step: Pick 3 posts from the recommended reading list and take notes on key disagreements
links:
  - https://alignmentforum.org
  - https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ
when_to_suggest: When they want to engage with cutting-edge thinking and have some technical background.
when_not_to_suggest: If they need more foundational context first or prefer structured courses.
---

## When to suggest
When they want to engage with cutting-edge thinking and have some technical background.

## When not to suggest
If they need more foundational context first or prefer structured courses.

## Notes
Read seminal posts on the Alignment Forum to understand current research debates.
