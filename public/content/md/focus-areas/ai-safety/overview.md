# GENERATED FROM VAULT — DO NOT EDIT. Source of truth: /vault

## What it’s trying to achieve (plain language)
Help ensure advanced AI systems don’t cause catastrophic harm, by improving how we build/evaluate/govern/deploy powerful AI (technical safety + governance/policy).

## What kinds of work exist here
- Technical research (alignment, interpretability, evals, robustness)
- Research engineering (tools, eval pipelines, infra for safety)
- Governance / policy / strategy
- Security / preparedness / model risk
- Ops / programs / field-building
- Communications / writing

## What a good fit often looks like (signals)
- Motivated by high-stakes downside risk + can tolerate uncertainty/debate
- Drawn to either: technical challenge, governance complexity, or enabling infrastructure/community
- OK with long feedback loops (or choose Experiment C to make it concrete fast)
