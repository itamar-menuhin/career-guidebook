[
  {
    "id": "ai-safety",
    "name": "AI Safety",
    "overview": "AI safety research aims to ensure that advanced AI systems remain beneficial and aligned with human values. The field spans technical research (alignment, interpretability, robustness), governance (policy, coordination), and field-building (education, community).",
    "roleShapes": [
      "Technical Alignment Researcher - working on core alignment problems like reward modeling, scalable oversight, interpretability",
      "ML Engineer at Safety Lab - implementing and testing alignment techniques in large models",
      "Governance & Policy - shaping AI regulation, international coordination, corporate policies",
      "Field Builder - running programs, funding, communications to grow the safety ecosystem",
      "Evaluations & Red-teaming - assessing model capabilities and finding failure modes"
    ],
    "fitSignals": [
      "You find yourself naturally drawn to thinking about worst-case scenarios and how to prevent them",
      "You have a strong technical background (ML, math, CS) or policy experience",
      "You can maintain motivation working on problems with uncertain or long-term payoffs",
      "You're comfortable with the philosophical complexity of defining \"beneficial AI\"",
      "You thrive in research environments with high autonomy"
    ],
    "buckets": {
      "quickTaste": {
        "title": "Quick taste (≈1 hour)",
        "description": "Get oriented with foundational readings that explain why AI safety matters and what the field is trying to solve.",
        "cardIds": [
          "alignment-forum-reading",
          "80k-ai-safety-guide"
        ],
        "inlineGuidance": "Start with the 80K guide for the career lens, then read one Alignment Forum post to see the technical depth."
      },
      "deeperDive": {
        "title": "Deeper dive (2–6 hours)",
        "description": "Commit a few hours to structured learning that builds conceptual foundations.",
        "cardIds": [
          "aisf-fundamentals"
        ],
        "inlineGuidance": "The AI Safety Fundamentals course is the gold standard intro. You can self-study the materials even without joining a cohort."
      },
      "handsOn": {
        "title": "Hands-on trial",
        "description": "Test your fit with programs that involve doing actual research or engineering work.",
        "cardIds": [
          "mats-program",
          "arena-program",
          "ai-safety-camp"
        ],
        "inlineGuidance": "These programs have different levels of selectivity. ARENA is great for building engineering skills; MATS pairs you with a mentor for research."
      },
      "jobBoard": {
        "title": "Job board scan (real roles)",
        "description": "See what organizations are actually hiring for to understand what skills are in demand.",
        "cardIds": [
          "anthropic-jobs",
          "redwood-jobs"
        ],
        "inlineGuidance": "Look at senior and junior roles to understand the skill trajectory. Even if you're not ready to apply, this shows what to aim for."
      }
    },
    "curatedCardIds": [
      "aisf-fundamentals",
      "mats-program",
      "arena-program",
      "alignment-forum-reading",
      "80k-ai-safety-guide",
      "anthropic-jobs"
    ],
    "peopleToTalkToPrompts": [
      "Reach out to AISF alumni in your city for casual coffee chats",
      "Attend AI safety meetups or virtual events to meet researchers",
      "Message researchers whose work you've read - many are surprisingly responsive",
      "Connect with MATS/ARENA alumni for program-specific advice"
    ],
    "commonConfusions": [
      "AI safety vs AI ethics - safety focuses on technical alignment and catastrophic risks; ethics is broader",
      "You don't need to be a genius ML researcher - the field needs diverse skills including ops, comms, policy",
      "Many paths in - career changers are common and valued for bringing fresh perspectives"
    ]
  }
]