[
  {
    "id": "ai-safety",
    "name": "AI Safety",
    "overviewPath": "content/md/focus-areas/ai-safety/overview.md",
    "overviewExcerpt": "AI safety research aims to ensure that advanced AI systems remain beneficial and aligned with human values.",
    "roleShapes": [
      "Technical Alignment Researcher - working on core alignment problems like reward modeling, scalable oversight, interpretability",
      "ML Engineer at Safety Lab - implementing and testing alignment techniques in large models",
      "Governance & Policy - shaping AI regulation, international coordination, corporate policies",
      "Field Builder - running programs, funding, communications to grow the safety ecosystem",
      "Evaluations & Red-teaming - assessing model capabilities and finding failure modes"
    ],
    "fitSignals": [
      "You find yourself naturally drawn to thinking about worst-case scenarios and how to prevent them",
      "You have a strong technical background (ML, math, CS) or policy experience",
      "You can maintain motivation working on problems with uncertain or long-term payoffs",
      "You're comfortable with the philosophical complexity of defining \"beneficial AI\"",
      "You thrive in research environments with high autonomy"
    ],
    "buckets": {
      "quickTaste": {
        "title": "Quick taste (\u22481 hour)",
        "description": "Get oriented with foundational readings that explain why AI safety matters and what the field is trying to solve.",
        "cardIds": [
          "alignment-forum-reading",
          "80k-ai-safety-guide"
        ],
        "inlineGuidancePath": "content/md/focus-areas/ai-safety/buckets/quick-taste.md"
      },
      "deeperDive": {
        "title": "Deeper dive (2\u20136 hours)",
        "description": "Commit a few hours to structured learning that builds conceptual foundations.",
        "cardIds": [
          "aisf-fundamentals"
        ],
        "inlineGuidancePath": "content/md/focus-areas/ai-safety/buckets/deeper-dive.md"
      },
      "handsOn": {
        "title": "Hands-on trial",
        "description": "Test your fit with programs that involve doing actual research or engineering work.",
        "cardIds": [
          "arena",
          "mats"
        ],
        "inlineGuidancePath": "content/md/focus-areas/ai-safety/buckets/hands-on.md"
      },
      "jobBoard": {
        "title": "Jobs to watch",
        "description": "Stay on top of hiring for safety labs, research orgs, and governance roles.",
        "cardIds": [
          "apart-research-jobs",
          "manifold",
          "80k-jobs-board"
        ],
        "inlineGuidancePath": "content/md/focus-areas/ai-safety/buckets/job-board.md"
      }
    },
    "curatedCardIds": [
      "80k-ai-safety-guide",
      "aisf-fundamentals",
      "alignment-forum-reading",
      "arena",
      "mats"
    ],
    "peopleToTalkToPrompts": [
      "Who is doing alignment work most similar to what you want to do?",
      "Is there someone who recently transitioned into AI safety from a comparable background?",
      "Who can give feedback on your first small steps?"
    ],
    "commonConfusions": [
      "Assuming all AI governance roles are DC-based policy jobs",
      "Thinking you need a PhD before contributing to alignment research"
    ]
  }
]
