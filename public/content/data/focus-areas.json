[
  {
    "id": "ai-safety",
    "name": "AI Safety",
    "overviewPath": "content/md/focus-areas/ai-safety/overview.md",
    "overviewExcerpt": "AI safety research aims to ensure that advanced AI systems remain beneficial and aligned with human values.",
    "roleShapes": [
      "Technical Alignment Researcher - working on core alignment problems like reward modeling, scalable oversight, interpretability",
      "ML Engineer at Safety Lab - implementing and testing alignment techniques in large models",
      "Governance & Policy - shaping AI regulation, international coordination, corporate policies",
      "Field Builder - running programs, funding, communications to grow the safety ecosystem",
      "Evaluations & Red-teaming - assessing model capabilities and finding failure modes"
    ],
    "fitSignals": [
      "You find yourself naturally drawn to thinking about worst-case scenarios and how to prevent them",
      "You have a strong technical background (ML, math, CS) or policy experience",
      "You can maintain motivation working on problems with uncertain or long-term payoffs",
      "You're comfortable with the philosophical complexity of defining \"beneficial AI\"",
      "You thrive in research environments with high autonomy"
    ],
    "buckets": {
      "quickTaste": {
        "title": "Quick taste (≈1 hour)",
        "description": "Start with the 80K guide for the career lens, then read one Alignment Forum post to see the technical depth.",
        "cardIds": [
          "80k-ai-safety-guide",
          "alignment-forum-reading"
        ],
        "inlineGuidancePath": "content/md/focus-areas/ai-safety/buckets/quick-taste.md",
        "curatedCardIds": [
          "alignment-forum-reading",
          "80k-ai-safety-guide"
        ]
      },
      "deeperDive": {
        "title": "Deeper dive (2–6 hours)",
        "description": "The AI Safety Fundamentals course is the gold standard intro. You can self-study the materials even without joining a cohort.",
        "cardIds": [
          "aisf-fundamentals"
        ],
        "inlineGuidancePath": "content/md/focus-areas/ai-safety/buckets/deeper-dive.md",
        "curatedCardIds": [
          "aisf-fundamentals"
        ]
      },
      "handsOn": {
        "title": "Hands-on trial",
        "description": "These programs have different levels of selectivity. ARENA is great for building engineering skills; MATS pairs you with a mentor for research.",
        "cardIds": [
          "ai-safety-camp",
          "arena-program",
          "mats-program"
        ],
        "inlineGuidancePath": "content/md/focus-areas/ai-safety/buckets/hands-on.md",
        "curatedCardIds": [
          "ai-safety-camp",
          "arena-program",
          "mats-program"
        ]
      },
      "jobBoard": {
        "title": "Job board scan (real roles)",
        "description": "Look at senior and junior roles to understand the skill trajectory. Even if you're not ready to apply, this shows what to aim for.",
        "cardIds": [
          "anthropic-jobs",
          "redwood-jobs"
        ],
        "inlineGuidancePath": "content/md/focus-areas/ai-safety/buckets/job-board.md",
        "curatedCardIds": [
          "anthropic-jobs",
          "redwood-jobs"
        ]
      }
    },
    "curatedCardIds": [
      "alignment-forum-reading",
      "80k-ai-safety-guide",
      "aisf-fundamentals",
      "ai-safety-camp",
      "arena-program",
      "mats-program",
      "anthropic-jobs",
      "redwood-jobs"
    ],
    "peopleToTalkToPrompts": [
      "Who is doing alignment work most similar to what you want to do?",
      "Is there someone who recently transitioned into AI safety from a comparable background?",
      "Who can give feedback on your first small steps?"
    ],
    "commonConfusions": [
      "Assuming all AI governance roles are DC-based policy jobs",
      "Thinking you need a PhD before contributing to alignment research"
    ]
  }
]
